---
title: "Exploratory Analysis"
author: "Tessa Senders"
date: "12/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries, warning=FALSE, message=FALSE}
library(tidyverse)
library(stringr)
library(leaps)
```


```{r load data}
hate_crimes_df <- read.csv("./data/HateCrimes.csv") %>%
  janitor::clean_names() %>%
  rename(perc_pop_hs = perc_population_with_high_school_degree) %>% 
  mutate(hate_crimes_per_100k_splc = as.numeric(hate_crimes_per_100k_splc)) %>%
  drop_na(hate_crimes_per_100k_splc) %>%
  mutate(unemployment = as_factor(unemployment),
         urbanization = as_factor(urbanization))
```

4 Missing states (Hawaii, North Dakota, South Dakota, Wyoming)

Percent Non Citizen Maine is missing


```{r descriptive stats}
hate_crimes_df %>%
  ggplot(aes(x = hate_crimes_per_100k_splc)) +
  geom_density()
```


```{r attempt transformation of outcome}
hate_crimes_df %>%
  mutate(hate_crimes_per_100k_splc = log(hate_crimes_per_100k_splc)) %>%
  ggplot(aes(x = hate_crimes_per_100k_splc)) +
  geom_density()
  
```


Add Box Cox Transformation Here


```{r add log transform}
hate_crimes_df <- hate_crimes_df %>%
  mutate(hate_crimes_log = log(hate_crimes_per_100k_splc))
```

```{r box plot}
hate_crimes_df %>%
  mutate(state = fct_reorder(state, hate_crimes_log)) %>%
ggplot(aes(x = state, y = hate_crimes_log)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust=1))
```

DESCRIPTIVE STATISTICS 
Table 1
More boxplots
etc.
Box Cox
2 people:Stella, Vasili


Regress given covariate (original model given by article)

```{r}
summary(lm(hate_crimes_per_100k_splc ~ . - state - hate_crimes_log, data = hate_crimes_df))
```

Yes, it does seem that the Gini index (income inequality) is the most significant predictor of hate crime and percent population with a HS degree is the only other significant predictor. Both of these results agree with the article.


```{r}
summary(lm(hate_crimes_log ~ . - state - hate_crimes_per_100k_splc, data = hate_crimes_df))
```

Here we re-ran the same model, but using log of the output. We obtain the same results in terms of sigificant variables. 

### Put modeling results for just gini index as variable and log hate crimes as outcome

Lets do research to pick other covariates to include in our model

Variables shown to be statistically significant in existent literature: 

- Race, religion, sexual orientation - Source: Study of Literature And Legislation on Hate Crime in America (147 page report for the US Justice Department) (https://www.ncjrs.gov/pdffiles1/nij/grants/210300.pdf)
- Urbanization/population density, economic considerations (median income, poverty level, job availability), cultural and education factors - Source: US FBI website (https://ucr.fbi.gov/hate-crime/2011/resources/variables-affecting-crime)

Can also do a backwards stepwise selection to come up with a preliminary model and then add/subtract from that too

```{r model_stuff, message = FALSE, warning = FALSE}
hate_crimes_df = hate_crimes_df %>%
  drop_na()

# backwards selection

mult.fit <- lm(hate_crimes_log ~ . - state - hate_crimes_per_100k_splc, data = hate_crimes_df)
step(mult.fit, direction = 'backward')

## backwards selection found only gini index and hs education to be significant
bw_mod = lm(formula = hate_crimes_log ~ perc_pop_hs + gini_index, 
    data = hate_crimes_df)
summary(bw_mod)

##backwards selection pre-excluding pop hs...does it include median income???
mult.fit.2 <- lm(hate_crimes_log ~ . - state - hate_crimes_per_100k_splc -perc_pop_hs, data = hate_crimes_df)
step(mult.fit.2, direction = 'backward')

bw_mod_no_hs = lm(formula = hate_crimes_log ~ median_household_income + gini_index, 
    data = hate_crimes_df)
summary(bw_mod_no_hs)

summary(lm(formula = hate_crimes_log ~ median_household_income +perc_pop_hs + gini_index, 
    data = hate_crimes_df))

# forward selection

mult.fit <- lm(hate_crimes_log ~ . - state - hate_crimes_per_100k_splc, data = hate_crimes_df)
step(mult.fit, direction = 'forward')

## forward selection found urbanization, unemployment, income, percent hs education, percent non-citizen, percent non-white and gini index to be significant
fw_mod = lm(formula = hate_crimes_log ~ perc_pop_hs + gini_index + unemployment + urbanization + median_household_income + perc_non_citizen + perc_non_white, data = hate_crimes_df)
summary(fw_mod)



perc_race1 = lm(formula = hate_crimes_log ~ perc_pop_hs + gini_index + perc_non_white + perc_non_citizen, 
    data = hate_crimes_df)
summary(perc_race1)

perc_race2 = lm(formula = hate_crimes_log ~ perc_pop_hs + gini_index + perc_non_citizen, 
    data = hate_crimes_df)
summary(perc_race2)

perc_race3 = lm(formula = hate_crimes_log ~ perc_pop_hs + gini_index + perc_non_white, 
    data = hate_crimes_df)
summary(perc_race3)



# Printing the 2 best models of each size, using the Cp criterion:
hc = hate_crimes_df %>%
  drop_na() %>%
  dplyr::select(-hate_crimes_per_100k_splc, -state) %>%
  mutate(unemployment = as.numeric(unemployment),
         urbanization = as.numeric(urbanization))
leaps::leaps(x = hc[,1:7], y = hc[,8], nbest=2, method="Cp")

# Printing the 2 best models of each size, using the adjusted R squared criterion:
hate_crimes_df %>%
  drop_na() %>%
  dplyr::select(-hate_crimes_per_100k_splc, -state) %>%
  mutate(unemployment = as.numeric(unemployment),
         urbanization = as.numeric(urbanization))
leaps::leaps(x = hc[,1:7], y = hc[,8], nbest=2, method="adjr2")

# Summary of models for each size (one model per size)
# Function regsubsets() performs a subset slection by identifying the "best" model that contains a certain number of predictors. By default "best" is chosen using SSE/RSS (smaller is better).

b <- regsubsets(hate_crimes_log ~ ., data=hc)
summary(b)

```
Article supports percent non-white as an important variable (for college campuses): https://journals-sagepub-com.proxy.lib.umich.edu/doi/full/10.1177/1043986214536666?utm_source=summon&utm_medium=discovery-provider

Anti-hispanic and religious hate crimes on the rise: https://apnews.com/article/hate-crimes-rise-fbi-data-ebbcadca8458aba96575da905650120d

Make the association matrix and compare covariates-look for multicollinearity
3 people: Lily, Caroline, Tessa

Correlation matrix of all numeric variables with each other:

```{r cor mat}
hate_crimes_df %>% 
  drop_na(perc_non_citizen) %>%
  dplyr::select(!(state:urbanization)) %>% 
  cor() %>% 
  round(.,3)
```

Out of the continuous variables, percent non-citizen and percent non-white has a correlation coefficient of 0.753, median household income and percentage of population with a HS degree has a correlation coefficient of 0.651, both of which may suggest multi-collinearity. All other variables do not suggest multi-collinearity.

Calculate VIFs for multicollinearity

Drop either perc non citizen or per non white
Drop either median household income or perc pop with HS degree-drop median household income due to multicollinearity

Test a model with interactions between each of the categorical variables and all the continuous variables-(pre-eliminate some continuous variables based on previous analysis).  Interaction between each categorical variable and the perc non white and perc non citizen

Stratify based on significant interactions and comment

Model diagnostics

Duplicate code with and without DC and comment on the effect of the outlier.

Final writeup!








